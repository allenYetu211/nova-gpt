{
    "sourceFile": "src/fetchs/OpenAI.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1682010275399,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1682010332691,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,9 +172,10 @@\n     })\n \n     res.on('end', () => {\n       const tokensUsed =\n-        countTokens(submitMessages.map((m) => m.content).join('\\n')) +\n+        // countTokens(submitMessages.map((m) => m.content).join('\\n')) +\n+        countTokens(messages.map((m) => m.content).join('\\n')) +\n         countTokens(buffer)\n \n       endCallback?.(tokensUsed)\n     })\n"
                }
            ],
            "date": 1682010275399,
            "name": "Commit-0",
            "content": "/*\n * @Author: Allen OYang\n * @Email:  allenwill211@gmail.com\n * @Date: 2023-04-19 10:58:17\n * @LastEditTime: 2023-04-21 01:04:35\n * @LastEditors: Allen OYang allenwill211@gmail.com\n * @FilePath: /speak-gpt/src/fetchs/OpenAI.ts\n */\nimport { IncomingMessage } from 'http'\nimport https from 'https'\nimport axios from 'axios'\nimport { Message } from '@/stores/ChatStore'\nimport { truncateMessages, countTokens } from '@/models/ChatMessage'\nimport { getModelInfo } from '@/models/ModelsAccount'\n\nconst fetchOpenAIData = async (url: string, key: string) => {\n  try {\n    const { data } = await axios.get(url, {\n      headers: {\n        Authorization: `Bearer ${key}`,\n      },\n    })\n    return data\n  } catch (e) {\n    console.error(e)\n  }\n}\n\nexport const fetchModels = async (key: string) => {\n  try {\n    const res = await fetchOpenAIData('https://api.openai.com/v1/models', key)\n    return res\n  } catch (e) {\n    console.warn(e)\n    return []\n  }\n}\n\nexport async function _streamCompletion(\n  payload: string,\n  apiKey: string,\n  abortController?: AbortController,\n  callback?: ((res: IncomingMessage) => void) | undefined,\n  errorCallback?: ((res: IncomingMessage, body: string) => void) | undefined,\n) {\n  const req = https.request(\n    {\n      hostname: 'api.openai.com',\n      port: 443,\n      path: '/v1/chat/completions',\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        Authorization: `Bearer ${apiKey}`,\n      },\n      signal: abortController?.signal,\n    },\n    (res) => {\n      if (res.statusCode !== 200) {\n        let errorBody = ''\n        res.on('data', (chunk) => {\n          errorBody += chunk\n        })\n        res.on('end', () => {\n          errorCallback?.(res, errorBody)\n        })\n        return\n      }\n      callback?.(res)\n    },\n  )\n\n  req.write(payload)\n\n  req.end()\n}\n\ninterface ChatCompletionParams {\n  model: string\n  temperature: number\n  top_p: number\n  n: number\n  stop: string\n  max_tokens: number\n  presence_penalty: number\n  frequency_penalty: number\n  logit_bias: string\n}\n\nconst paramKeys = [\n  'model',\n  'temperature',\n  'top_p',\n  'n',\n  'stop',\n  'max_tokens',\n  'presence_penalty',\n  'frequency_penalty',\n  'logit_bias',\n]\n\nexport async function streamCompletion(\n  messages: Message[],\n  params: ChatCompletionParams,\n  apiKey: string,\n  abortController?: AbortController,\n  callback?: ((res: IncomingMessage) => void) | undefined,\n  endCallback?: ((tokensUsed: number) => void) | undefined,\n  errorCallback?: ((res: IncomingMessage, body: string) => void) | undefined,\n) {\n  const modelInfo = getModelInfo(params.model)\n\n  // Truncate messages to fit within maxTokens parameter\n  // const submitMessages = truncateMessages(\n  //   messages,\n  //   modelInfo.maxTokens,\n  //   params.max_tokens,\n  // )\n\n  const submitParams = Object.fromEntries(\n    Object.entries(params).filter(([key]) => paramKeys.includes(key)),\n  )\n\n  const payload = JSON.stringify({\n    // messages: submitMessages.map(({ role, content }) => ({ role, content })),\n    messages: messages.map(({ role, content }) => ({ role, content })),\n    stream: true,\n    ...{\n      ...submitParams,\n      logit_bias: JSON.parse(params.logit_bias || '{}'),\n      // 0 == unlimited\n      max_tokens: params.max_tokens || undefined,\n    },\n  })\n\n  let buffer = ''\n\n  const successCallback = (res: IncomingMessage) => {\n    res.on('data', (chunk) => {\n      if (abortController?.signal.aborted) {\n        res.destroy()\n        endCallback?.(0)\n        return\n      }\n\n      // Split response into individual messages\n      const allMessages = chunk.toString().split('\\n\\n')\n      for (const message of allMessages) {\n        // Remove first 5 characters (\"data:\") of response\n        const cleaned = message.toString().slice(5)\n\n        if (!cleaned || cleaned === ' [DONE]') {\n          return\n        }\n\n        let parsed\n        try {\n          parsed = JSON.parse(cleaned)\n        } catch (e) {\n          console.error(e)\n          return\n        }\n\n        const content = parsed.choices[0]?.delta?.content\n        if (content === undefined) {\n          continue\n        }\n        buffer += content\n\n        callback?.(content)\n      }\n    })\n\n    res.on('end', () => {\n      const tokensUsed =\n        countTokens(submitMessages.map((m) => m.content).join('\\n')) +\n        countTokens(buffer)\n\n      endCallback?.(tokensUsed)\n    })\n  }\n\n  return _streamCompletion(\n    payload,\n    apiKey,\n    abortController,\n    successCallback,\n    errorCallback,\n  )\n}\n"
        }
    ]
}