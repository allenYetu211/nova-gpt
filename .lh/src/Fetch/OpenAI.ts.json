{
    "sourceFile": "src/Fetch/OpenAI.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1681873767307,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1681877139045,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,9 @@\n     console.error(e);\n   }\n }\n \n-\n-const fetchModels = async (key: string) => {\n+export const fetchModels = async (key: string) => {\n   try {\n     const res = await fetchOpenAIData('https://api.openai.com/v1/chat/completions', key)\n     return res;\n   } catch(e) {\n"
                },
                {
                    "date": 1681878606393,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n }\n \n export const fetchModels = async (key: string) => {\n   try {\n-    const res = await fetchOpenAIData('https://api.openai.com/v1/chat/completions', key)\n+    const res = await fetchOpenAIData('https://api.openai.com/v1/models', key)\n     return res;\n   } catch(e) {\n     console.warn(e);\n     return []\n"
                },
                {
                    "date": 1681896949267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,12 @@\n+/*\n+ * @Author: Allen OYang\n+ * @Email:  allenwill211@gmail.com\n+ * @Date: 2023-04-19 10:58:17\n+ * @LastEditTime: 2023-04-19 12:30:06\n+ * @LastEditors: Allen OYang allenwill211@gmail.com\n+ * @FilePath: /speak-gpt/src/Fetch/OpenAI.ts\n+ */\n import axios from 'axios';\n \n \n const fetchOpenAIData = async (url: string, key: string) => {\n@@ -22,5 +30,158 @@\n   } catch(e) {\n     console.warn(e);\n     return []\n   }\n+}\n+\n+\n+export async function _streamCompletion(\n+  payload: string,\n+  apiKey: string,\n+  abortController?: AbortController,\n+  callback?: ((res: IncomingMessage) => void) | undefined,\n+  errorCallback?: ((res: IncomingMessage, body: string) => void) | undefined\n+) {\n+  const req = https.request(\n+    {\n+      hostname: \"api.openai.com\",\n+      port: 443,\n+      path: \"/v1/chat/completions\",\n+      method: \"POST\",\n+      headers: {\n+        \"Content-Type\": \"application/json\",\n+        Authorization: `Bearer ${apiKey}`,\n+      },\n+      signal: abortController?.signal,\n+    },\n+    (res) => {\n+      if (res.statusCode !== 200) {\n+        let errorBody = \"\";\n+        res.on(\"data\", (chunk) => {\n+          errorBody += chunk;\n+        });\n+        res.on(\"end\", () => {\n+          errorCallback?.(res, errorBody);\n+        });\n+        return;\n+      }\n+      callback?.(res);\n+    }\n+  );\n+\n+  req.write(payload);\n+\n+  req.end();\n+}\n+\n+interface ChatCompletionParams {\n+  model: string;\n+  temperature: number;\n+  top_p: number;\n+  n: number;\n+  stop: string;\n+  max_tokens: number;\n+  presence_penalty: number;\n+  frequency_penalty: number;\n+  logit_bias: string;\n+}\n+\n+const paramKeys = [\n+  \"model\",\n+  \"temperature\",\n+  \"top_p\",\n+  \"n\",\n+  \"stop\",\n+  \"max_tokens\",\n+  \"presence_penalty\",\n+  \"frequency_penalty\",\n+  \"logit_bias\",\n+];\n+\n+export async function streamCompletion(\n+  messages: Message[],\n+  params: ChatCompletionParams,\n+  apiKey: string,\n+  abortController?: AbortController,\n+  callback?: ((res: IncomingMessage) => void) | undefined,\n+  endCallback?: ((tokensUsed: number) => void) | undefined,\n+  errorCallback?: ((res: IncomingMessage, body: string) => void) | undefined\n+) {\n+  const modelInfo = getModelInfo(params.model);\n+\n+  // Truncate messages to fit within maxTokens parameter\n+  const submitMessages = truncateMessages(\n+    messages,\n+    modelInfo.maxTokens,\n+    params.max_tokens\n+  );\n+\n+  const submitParams = Object.fromEntries(\n+    Object.entries(params).filter(([key]) => paramKeys.includes(key))\n+  );\n+\n+  const payload = JSON.stringify({\n+    messages: submitMessages.map(({ role, content }) => ({ role, content })),\n+    stream: true,\n+    ...{\n+      ...submitParams,\n+      logit_bias: JSON.parse(params.logit_bias || \"{}\"),\n+      // 0 == unlimited\n+      max_tokens: params.max_tokens || undefined,\n+    },\n+  });\n+\n+  let buffer = \"\";\n+\n+  const successCallback = (res: IncomingMessage) => {\n+    res.on(\"data\", (chunk) => {\n+      if (abortController?.signal.aborted) {\n+        res.destroy();\n+        endCallback?.(0);\n+        return;\n+      }\n+\n+      // Split response into individual messages\n+      const allMessages = chunk.toString().split(\"\\n\\n\");\n+      for (const message of allMessages) {\n+        // Remove first 5 characters (\"data:\") of response\n+        const cleaned = message.toString().slice(5);\n+\n+        if (!cleaned || cleaned === \" [DONE]\") {\n+          return;\n+        }\n+\n+        let parsed;\n+        try {\n+          parsed = JSON.parse(cleaned);\n+        } catch (e) {\n+          console.error(e);\n+          return;\n+        }\n+\n+        const content = parsed.choices[0]?.delta?.content;\n+        if (content === undefined) {\n+          continue;\n+        }\n+        buffer += content;\n+\n+        callback?.(content);\n+      }\n+    });\n+\n+    res.on(\"end\", () => {\n+      const tokensUsed =\n+        countTokens(submitMessages.map((m) => m.content).join(\"\\n\")) +\n+        countTokens(buffer);\n+\n+      endCallback?.(tokensUsed);\n+    });\n+  };\n+\n+  return _streamCompletion(\n+    payload,\n+    apiKey,\n+    abortController,\n+    successCallback,\n+    errorCallback\n+  );\n }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1681897805015,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,13 +1,18 @@\n /*\n  * @Author: Allen OYang\n  * @Email:  allenwill211@gmail.com\n  * @Date: 2023-04-19 10:58:17\n- * @LastEditTime: 2023-04-19 12:30:06\n+ * @LastEditTime: 2023-04-19 17:50:01\n  * @LastEditors: Allen OYang allenwill211@gmail.com\n  * @FilePath: /speak-gpt/src/Fetch/OpenAI.ts\n  */\n+import { IncomingMessage } from \"http\";\n+import https from 'https';\n import axios from 'axios';\n+import { Message } from '@/stores/ChatStore'\n+import { truncateMessages, countTokens } from '@/models/ChatMessage'\n+import { getModelInfo } from '@/models/ModelsAccount'\n \n \n const fetchOpenAIData = async (url: string, key: string) => {\n \n@@ -26,9 +31,9 @@\n export const fetchModels = async (key: string) => {\n   try {\n     const res = await fetchOpenAIData('https://api.openai.com/v1/models', key)\n     return res;\n-  } catch(e) {\n+  } catch (e) {\n     console.warn(e);\n     return []\n   }\n }\n"
                }
            ],
            "date": 1681873767307,
            "name": "Commit-0",
            "content": "import axios from 'axios';\n\n\nconst fetchOpenAIData = async (url: string, key: string) => {\n\n  try {\n    const { data } = await axios.get(url, {\n      headers: {\n        'Authorization': `Bearer ${key}`\n      }\n    })\n    return data;\n  } catch (e) {\n    console.error(e);\n  }\n}\n\n\nconst fetchModels = async (key: string) => {\n  try {\n    const res = await fetchOpenAIData('https://api.openai.com/v1/chat/completions', key)\n    return res;\n  } catch(e) {\n    console.warn(e);\n    return []\n  }\n}"
        }
    ]
}